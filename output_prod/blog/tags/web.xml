<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[Ciaran McNulty]]></title>
    <link href="/blog/tags/web.xml" rel="self"/>
    <link href="/"/>
    <updated>2015-07-05T13:56:44+01:00</updated>
    <id>/</id>
        <generator uri="http://sculpin.io/">Sculpin</generator>
            <entry>
            <title type="html"><![CDATA[Always declare a media type in your CSS links]]></title>
            <link href="/always-declare-a-media-type-in-your-css-links"/>
            <updated>2010-07-08T00:00:00+01:00</updated>
            <id>/always-declare-a-media-type-in-your-css-links</id>
            <content type="html"><![CDATA[<p>An odd one today at work, a site had the following:</p>

<code><pre>
&lt;link rel="stylesheet" href="styles.css" /&gt;
&lt;link rel="stylesheet" href="print.css" media="print" /&gt;
</pre></code>

<p>What was puzzling was that when you printed the page from Firefox, none of the styles from <tt>styles.css</tt> were being applied. I for one had assumed that it would be inherited.</p>

<p>It turns out that it's not clear what the default media value for the <tt>link</tt> element is.  <a href="http://www.w3.org/TR/REC-html40/present/styles.html#adef-media">The HTML4 spec</a> says:</p>

<blockquote>
This attribute specifies the intended destination medium for style information. It may be a single media descriptor or a comma-separated list. The default value for this attribute is "screen".
</blockquote>

<p>Pretty clear-cut, although maybe not what I expected - looks like if the <tt>media</tt> attribute is missing the styles only apply to screens. But hang on, look what <a href="http://dev.w3.org/html5/spec/Overview.html#attr-link-media">the HTML5 spec</a> says:</p>

<blockquote>
The default, if the media attribute is omitted, is "all", meaning that by default links apply to all media.
</blockquote>

<p>That's a more sensible default of course, but it's unusual to see the HTML5 spec completely change behaviour like this. The fact it has changed indicates that browsers are (currently) being inconsistent about which value to default to.</p>

<p>For now, it seems sensible to <em>always</em> declare a media value in your <tt>link</tt>s, and not make assumptions like we did!</p>

]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Clarifying Javascript-PHP communication using JSON-RPC]]></title>
            <link href="/clarifying-javascript-php-communication-using-json-rpc"/>
            <updated>2010-03-15T00:00:00+00:00</updated>
            <id>/clarifying-javascript-php-communication-using-json-rpc</id>
            <content type="html"><![CDATA[<p>I think of myself first and foremost as a PHP developer but serious sites are getting more and more JS-heavy as time goes on so it gets harder (and less pragmatic) to try and avoid dealing with JS<->PHP communication of some sort.</p>

<p>I'm a big advocate of RESTful design so tend to end up attempting to write scripts that do lots of GETs and POSTs (as appropriate) and parsing out whatever custom response format I've decided JSON requests will return.  It feels good - like I'm sticking to my principles and 'doing it right' but it's a long painful slog that can feel like self-flagellation at times.</p>

<p>It's also slow and hard to prototype - it's hard to argue in favour of some abstract design idea when it's making you take forever to generate simple tasks .  Sometimes when I feel lazy what I really want is a way of calling my PHP objects directly from the JS and not worrying about what's happening in the underlying HTTP, and that's what <a href="http://json-rpc.org">JSON-RPC</a> provides.</p>

<p>In this blog I'll be showing some simple examples of JSON-RPC in action but first let's look at the pros and cons.</p>

<h4>Why JSON-RPC is awesome:</h4>

<ul>
<li>It includes basic error handling as part of the protocol</li>
<li>It hides away all the client-server communications from me the developer</li>
<li>It looks a bit like SOAP so is fairly familiar</li>
<li>I don't have to spend time writing RESTful response formats and pondering URLs to get working code</li>
</ul>

<h4>Why JSON-RPC sucks:</h4>

<ul>
<li>It's not <a href="http://en.wikipedia.org/wiki/Rest">REST</a>ful - requests from the client come in as POSTs to on URL without a clear semantic 'resource' behind it</li>
<li>There may be specifics to the HTTP that aren't covered by the abstraction</li>
</ul>

<p>I suspect a lot of developers reading the lists above will be thinking "hell yeah, sounds good, how can I have it?"</p>

<h4>The protocol</h4>

<p>I won't go into all the ins and outs of the protocol but it's worth taking a quick look.  The basic paradigm is that the server has a bunch of methods that can be called by the client, that take different parameters.</p>

<p>An example request is a POST to the server that looks like this:</p>

<p><tt>{ "method": "greet", "params": ["Ciaran"], "id": 1234 }</tt></p>

<p>This is the PHP equivalent of $server->greet("Ciaran").  The ID parameter is used to match up requests and responses.  The response would look like this:</p>

<p><tt>{ "result": "Hello Ciaran", "error": null, "id": 1234 }</tt></p>

<p>So the result is a string literal with no error conditions - there's not much more to it than that to be honest, aside from error handling.</p>

<p>It would be fairly simple to implement either side of the protocol yourself on a project, but the real strength is of course that once something is a standard, the rest of the world goes to work on it and starts writing up libraries that lazy programmers like us can use! Let's take a look at two libraries, a Zend Framework one on the PHP end and a jQuery one on the Javascript end.</p>

<h4>JSON-RPC in PHP: Zend_Json_Server</h4>

<p><a href="http://framework.zend.com/manual/en/zend.json.server.html">Zend_Json_Server</a> is pretty simple to use, you will need a class that exposes and handles the methods you're going to be using that's properly docblock commented (this is used by the component to see the parameter and return types):</p>

<code><pre>class My_Service_Handler
{
    /**
    * @param string $name
    * @return string
    */
    public function greet($name)
    {
        return "Hello $name";
    }
}
</code></pre>

<p>You can then proxy JSON-RPC requests along into this object using the Zend Framework component, Zend_Json_Server:</p>

<code><pre>
$server = new Zend_Json_Server();
$server->setClass('My_Service_Handler');
$server->handle();
</pre></code>

<p>Now any requests like our examples above will get an appropriate response.  Again fairly straightforward stuff I think, and should be familiar to anyone who's used a SOAP server in PHP.  All of the complexity of decoding the response and encoding the request is handled by the component.</p>

<h4>JSON-RPC in JQuery: zendjsonrpc</h4>

<p>There are a huge number of jQuery plugins to do JSON-RPC, but <a href="http://plugins.jquery.com/project/zendjsonrpc">this one is specifically written to work with the Zend_Json_Server</a> so I figured it's as good an example as any.  If you look at the source code of the plugin it's fairly straightforward and I'm sure it'd be easy to write your own if you preferred.  The basic usage is:</p>

<code><pre>
var client = jQuery.Zend.jsonrpc({url: '/path/to/service/handler.php'});
var message = client.greet("Ciaran");
alert(message);
</pre></code>

<p>As you can see, once the client object is constructed with the service URL you as a developer have a local JS object that exposes the exact same methods the My_Service_Handler has.</p>

<h4>Overall</h4>

<p>What I really find attractive about the JSON-RPC protocol is that, well, someone else has implemented it for me in the two components above.  That means that I can stop worrying about AJAX and start pretending that my PHP object is somehow present locally in my JS and that I can just call its methods at will.</p>

<p>That's a really powerful abstraction. No doubt like most abstractions there are places where it leaks around the edges but the simplicity of the idea and the clarity it brings to JS<->PHP communication make it an approach I'll be certainly investigating for my next JS-heavy project.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Wag.gd - short URLs with a point]]></title>
            <link href="/wag-gd-short-urls-with-a-point"/>
            <updated>2009-12-29T00:00:00+00:00</updated>
            <id>/wag-gd-short-urls-with-a-point</id>
            <content type="html"><![CDATA[<p>Short URLs are a bit of a hot-button topic amongst devs - some think they're useful (like <a href="http://rdjs.co.uk/web/Implement-revcanonical-and-become-supercool/4" rel="friend">Russell</a>), some are wary of them (like <a href="http://ciaranmcnulty.com/blog/2009/04/rev-canonical-should-be-handled-with-care" rel="me">me</a>) and some, like my friend <a href="http://pointbeing.net/" rel="friend">Simon</a>, rail against them as a waste of time and resource.</p>

<p>Simon, however, is a sensible sort so rather than just making lots of noise about how they suck, he's gone away and tried to think of a sensible use case, then implemented it.</p>

<p>So take a look over at <a href="http://wag.gd">Wag.gd</a> if you're interested in mobile development.  <a href="http://pointbeing.net/weblog/2009/12/mobile-friendly-short-urls-with-waggd.html">Simon's written about it</a> far better than I could and it's certainly an interesting idea.</p>  ]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[FoWA London 2009 Round-up]]></title>
            <link href="/fowa-2009-london-round-up"/>
            <updated>2009-10-06T00:00:00+01:00</updated>
            <id>/fowa-2009-london-round-up</id>
            <content type="html"><![CDATA[   <div class="figure narrow">

<a href="http://www.flickr.com/photos/ciaranmcnulty/3986858821/" title="FoWA London by CiaranJMcNulty, on Flickr"><img src="http://farm4.static.flickr.com/3438/3986858821_73ba3fae6c_m.jpg" width="240" height="180" alt="FoWA London" /></a>

</div>



<p>I was lucky enough to go along to <a href="http://events.carsonified.com/fowa/2009/london/content"><abbr title="The Future of Web Applications">FoWA</abbr></a> this week with a couple of my colleagues, so I thought I'd do a quick round-up of the two days.</p>

<p>I really enjoyed the conference overall - Kensington Town Hall was pretty adequate for the size of conference, the afterparty at Orchid on the Thursday was fairly epic, and the talks left me really energised about the state of web applications in general.  The only slight blot on the whole thing was the terrible state of the WiFi, which was unusable on the first day and pretty flakey the second.</p>

<p>Rather than going through it in strict chronological order I've grouped the talks into broad themes. When the videos turn up online I'll try and link them through.</p>

<h4>Product Demos</h4>

<p>OK so FoWA talks tend to ostensibly be on some abstract theme, but really a hell of a lot are an excuse to show off some tech the speaker is involved in. There were a few that were really impressive.</p>

<h5>280 North</h5>

<img src="http://280north.com/i/280NorthLogoSmall.png" width="160" style="float: right; padding: 0 0 1em 1em; border: 0;" />

<p>This was a prime example.  <a href="http://tolmasky.com/">Francisco Tolmasky</a> from <a href="http://280north.com/">280 North</a> ran through their system for creating online apps, and it's safe to say that it blew the audience's socks off.  <a href="http://objective-j.org/">Cappuccino</a> is a Javascript application framework while <a href="http://280north.com/blog/2009/02/announcing-atlas/">Atlas</a> offers drag-and-drop interface and behaviour design for that framework.  The framework seems incredibly advanced, with pre-built interface elements for windowing and some amazing drag/drop functionality that makes you forget you're in a web environment.</p>

<p>In short, you can pretty much build a whole web application graphically and just write some JS to control how it talks to your server, then export that as a JS/HTML interface.  It's all certainly jaw-dropping, and is already being used to create some real apps like 280 Slides.  Francisco was pretty clear that they've aimed Atlas squarely at the 'online app' end of the market - it's not for generating 'websites' - and from what I've seen the JS it produces degrades gracefully and interfaces with a server back-end really cleanly (essentially via any RESTful interface). </p>

<p>The kicker came when Francisco showed how Atlas now allows you to export your application as a native application for your users to run on their desktops without any significant code changes, allowing them to save to native files instead of remote servers.  Taking it a step further, he showed how a native Mac interface designed in Interface Builder could go the other way, and be imported into Cappuccino as a web interface.  I think that was the point at which my brain switched off and assumed I was dreaming.</p>

<p>I'd strongly recommend a look at <a href="http://280north.com/blog/2009/02/announcing-atlas/">their demo video</a> for a glimpse of the future in that end of the market.  I can certainly see web apps like ours becoming more and more like desktop applications in their interfaces, and the idea that we could write our site and then offer a desktop version with a unified codebase was pretty exciting.</p>

<h5>Ubuntu Enterprise Cloud</h5>

<img src="http://www.ubuntu.com/sites/all/themes/ubuntu09/logo.png" width="160" style="float: right; padding: 0 0 1em 1em; border: 0;" />

<p><a href="http://blog.gardeviance.org/">Simon Wardley</a>, billed as a freelancer, gave a great talk about the future of cloud computing.  He talked about the different types of cloud networks (private vs public) and about how dependencies between service providers can lead to a bit of a domino effect when a cloud vendor goes down.</p>

<p>For example, if someone builds an service on <a href="http://aws.amazon.com/ec2/">EC2</a>, then sells that service to another party who builds an app, who then has end customers, when Amazon has server issues the whole chain fails. His main thrust was that we were in a very early transitional phase from traditional to cloud computing and that to progress we need to have more compatibility between the different cloud platform vendors to allow portability, for instance in the example above maybe some of the webservers were on Amazon and the rest on another service.</p>

<p>To that end he talked about <a href="http://www.ubuntu.com/products/whatisubuntu/serveredition/cloud/UEC">Ubuntu Enterprise Cloud</a> - this is an open source server implementation that's EC2 compatible.  The cool thing there is that you can run your web app on your own server hardware the way you normally do, just with this extra virtualisation layer in place.  Then when your app goes viral and gets digg'd, slashdotted and blogged to death you can quickly duplicate it across EC2 or any other compatible hosted solutions.</p>

<p>It would be good if the existence of this open-source reference implementation helped push the EC2 VM format and APIs towards becoming a de facto standard.  We currently sometimes use some Amazon services at peak load times. It would be interesting to be able to have a mix of local and remote servers and datastores, but treat them all with the same API.</p>

<h5>Facebook and Six Apart</h5>

<img src="http://blogs.ft.com/techblog/files/2009/08/facebook-logo.jpg" width="160" style="float: right; padding: 0 0 1em 1em; border: 0;" />
<img src="http://upload.wikimedia.org/wikipedia/en/thumb/9/95/Sixapart_logo.svg/295px-Sixapart_logo.svg.png" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;" />

<p>Everyone nowadays seems to want to be the gatekeeper for social media, but <a href="http://www.facebook.com">Facebook</a> and <a href="http://sixapart.com">Six Apart</a> both showed their solutions, which are pretty different.</p>

<p><a href="http://www.facebook.com/cat">Cat Lee</a> from Facebook appeared to have been brainwashed as part of her induction.  Her talk was a bit over-corporate and was about on how you can add social functionality to your sites using <a href="http://developers.facebook.com/connect.php">Facebook Connect</a>.  It seems like a decent solution if you're a big site who don't mind 'partnering' with Facebook and accepting lots of their branding, but for a seamless solution it's a complete no-go without your app becoming part of some sort of Facebook ecosystem.</p>

<p>In contrast, the talk from Six Apart showed a similar sort of proposition, but one that's a totally open and free platform.  The <a href=""http://developer.typepad.com/documentation/>TypePad development API</a> they're exposing now can let you accept login from multiple points (including OpenID and even Facebook Connect!), can keep your user's social graphs on their servers, and even store items of content that the users want to share. </p>

<p>This seems ideal for plugging in 'social' functionality to sites without much backend - a good example is <a href="http://www.zacharyquinto.com/">Zachary Quinto's site</a> which is basically a brochure site with some TypePad API stuff dropped in, but it's actually pretty feature-rich for users.  I think the Quinto site, for instance, won't need a database as all the assets, logins, friendships and so forth are kept in TypePad.</p>

<h5>Vodafone Mobile Widgets</h5>

<img src="http://online.vodafone.co.uk/dispatch/Portal/SimpleGetFileServlet?dDocName=VF010304&revisionSelectionMethod=latestReleased&inline=false" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p><a href="http://twitter.com/sanjmatharu">Sanj Matharu</a> from <a href="http://vodafone.co.uk">Vodafone</a> was there to show their new mobile applications platform, <a href="http://www.vodafone360.com/en/web/home/index">Vodafone 360</a>, and a guy called Joel Moss from <a href="http://codaset.com/">Codaset</a> went through some of his own experiences developing for it.</p>

<p>The idea for their platform was interesting - apps are basically ZIP files containing HTML, CSS and Javascript that runs the app along with some XML manifests.  The Javascript then gets a lot of the phone's infrastructure exposed to it via a standardised API that presumably irons over all the differences in phone capabilities.  The example app they showed us was a fairly simple battery level monitor, but it's interesting to see that sort of data accessed via Javascript.</p>

<p>The idea is solid, but I have doubts about Vodafone's ability as an operator to push this further.  It's very 'branded' at the moment so maybe other operators would be slow to take it up, but it looks like all the technology involved is portable at least (the application uses the Opera engine to render the apps).</p>

<h5>The Guardian</h5>

<img src="http://i.thisis.co.uk/274079/promoBoxModule/images/655645/657461.jpg" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p><a href="http://guardian.co.uk">The Guardian</a> seem to be really committed to joining the online community rather than fighting against it like King Canute or, for instance, Ruper Murdoch.  Chris Thorpe took us through their new initiatives including <a href="http://www.guardian.co.uk/open-platform">open APIs</a> for finding and sharing Guardian content, open datasets of public information the paper has gathered and curated.</p>

<p>He also took us through their recent project for extracting MPs claim expenses from the PDFs they were published in and constructing a system for members of the public to audit and annotate them, flagging up interesting expense claims for review by the journos.  The interesting bit about this project was that it took remarkably few developer days (I think it was about 3 man days in total) and is hosted on EC2 rather than traditional servers.  Apparently it's so far cost only about &#8356;50.</p>

<h4>Building the future</h4>

<p>Aside from (and overlapping with) product demos, at FoWA you get a certain type of talk that's concerned with predicting how the web application industry will go in the future, and talking about coming technologies that will enable that.</p>

<h5>HTML 5</h5>

<img src="http://komplettie.files.wordpress.com/2009/08/opera-logo.jpg?w=300&h=262" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p><a href="http://www.brucelawson.co.uk/">Bruce Lawson</a> from <a href="http://www.opera.co.uk">Opera</a> took the stage to demo <a href="http://dev.w3.org/html5/spec/Overview.html">HTML5</a>.  To be honest he didn't go into much that anyone following HTML5's development wouldn't be familiar with but managed to do it in an incredibly engaging and enthusiastic style.  I think most people leaving the talk would have immediately gone and started building stuff in HTML5.

<h5>Javascript frameworks</h5>

<img src="http://sharkenergy.ca/magazine/wp-content/uploads/2009/05/twitter-logo-001.jpg" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p><a href="http://www.dustindiaz.com/">Dustin Diaz</a> from <a href="http://twitter.com">Twitter</a> gave a slightly frazzled but fast-paced talk about how awesome Javascript was.  Twitter use jQuery extensively, and he had some great things to say about frameworks in general that I think applied outside of the JS world. </p>

<p>He compared <a href="http://jquery.com">jQuery</a> to cocaine in a fairly longstanding and contrived metaphor, but basically doing a few lines every so often seem great, but if you have a friend who's doing line after line all day long you should take him aside and get him to seek help.</p>

<p>What interested me about Diaz was that he had come into this jQuery organisation but actually had a great pragmatic view of things.  Basically he said that you should use frameworks as the tools they are, for the things they're good at, but keep an eye on the bigger picture and not be afraid to stray outside of the constraints your framework imposes on you.</p>

<h5>Accessiblity</h5>

<img src="http://www.abilitynet.org.uk/images/ab_logo.gif" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p><a href="http://twitter.com/usa2day">Robin Christopherson</a> from <a href="http://www.abilitynet.org.uk">AbilityNet</a> spoke to us about accessibility, and to be honest I was ready for a yawnfest.  Accessibility is something that devs in particular can treat as something of a chore.  Quite to the contrary though, he was really interesting!  He took us through some popular sites via a screen reader, and it was evident that some (e.g. Facebook) were not doing a great job for their disabled users.

<p>Robin seemed to have plenty of stats to back up the fact that disabled users are a huge market online who are traditionally sidelined even though they tend to be more active online than 'able-bodied' users.</p>

<p>One thing that was food for thought was his anti-CAPTCHA stance - <a href="http://www.zurb.com/article/285/its-official-captchas-are-bad-for-business">they seem to exclude disabled users while only offering moderate spam catching</a>.  It's certainly worth thinking about whether the cost of looking through spam is less than the cost of the users you exclude!  </p>

<p>Something else I found pretty interesting, being in the industry I am, was he showed us a demo of <a href="http://www.annodex.net/~silvia/itext/elephant_no_skin.html">HTML5 accessible video</a>, with subtitles and audio description kept in sync with the video.</p>

<h5>Ruby on Rails</h5>

<img src="http://blog.eukhost.com/images/rails_logo.jpg" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p>As a PHP guy this wasn't of massive interest, but <a href="http://yehudakatz.com/">Yehuda Katz</a> gave a great talk about what to expect in Rails 3, which seems to be the highly successful rival Merb MVC system integrating with Ruby.  What I found most interesting about the talk was the fact he and his partner are doing Pair Programming on Rails, which is very rare in the OSS world.  He seemed to find it pretty valuable and while I'm still suspicious of true pair stuff, it was food for thought.</p>

<h4>Business tips</h4>

<p>Another sort of talk you get at FoWA is of the 'how to build a startup' type.  As a developer these were less relevant, but some still contained some interest.</p>

<h5>Virb</h5>

<img src="http://www.littlepaperplanes.com/virb.jpg" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p>The talk from <a href="http://chrislea.com/">Chris Lea</a> from <a href="http://virb.com">Virb</a> was probably the one I got the most from.  Ostensibly about scaling, he was very keen on the idea that developers need to recognise that they're part of real-life businesses, and be more pragmatic.  The most interesting point he made was that a scalable web application isn't one that's had man-hours thrown at it to squeeze out 10% less CPU usage, it's one where the costs of buying more CPU are known, the plan is in place, and the business already knows that the costs will be covered by the increased revenue when expansion happens.</p>

<h5>Digg / Freshbooks</h5>

<img src="http://www.citystyleandliving.com/socialnetworking/digg-logo.png" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />
<img src="http://www.freshbooks.com/images/freshbooks2.gif" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />


<p><a href="http://kevinrose.com/">Kevin Rose</a> from <a href="http://digg.com">Digg</a>  and <a href="http://www.michaelmcderment.com/">Mike McDerment</a> from <a href="http://freshbooks.com">Freshbooks</a> started things off with separate talks about growing your audience.  Rose's message was one of engagement with the audience, and that's something that Digg do incredibly well. McDerment hammered home the idea that to do that you need to have some really solid metrics and reporting in place to be able to achieve this.  It's easy to leave reporting as low-priority, then spend the next few years of your product cycle eating up the man days producing ad hoc reports.</p>

<h5>Spymaster</h5>

<img src="http://cache0.techcrunch.com/wp-content/uploads/2009/05/picture-77.png" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p><a href="http://www.chrisabad.com/">Chris Abad</a> from <a href="http://playspymaster.com/">Spymaster</a> had something like an alternate universe version of the same message, where getting users involved equates to spamifying their friends via Twitter.  I didn't have much time for him but he did have an interesting point about how after a big explosion like they had, you need to take stock of what you've got when the dust settles and think carefully about how to serve your core audience and make the model sustainable (and wow, people still seem to be playing Spymaster - who knew?).</p>

<h5>Revision3</h5>

<img src="http://cybernetnews.com/wp-content/uploads/2008/05/revision-3-logo.jpg" width="160" style="clear: right; float: right; padding: 0 0 1em 1em; border: 0;"  />

<p><a href="http://twitter.com/dlprager">David Prager</a> from <a href="http://revision3.com/">Revision3</a>'s theme was essentially that of the Long Tail where even very specialist interest products can gain an audience but with the twist that once you've succeeded in focussing on a small niche, you can then build on that success in a broader sense. I found this pretty convincing - the paradigmatic examples he used were Facebook (originally a University system) and Digg (originally tech news but now general interest stuff).  Coming from a company that concentrates on some mildly niche industries it was interesting stuff, it got me wondering about what we could do next, once we dominate our space!</p>

<h4>Overall</h4>

<p>Well, that's most of what I can remember from FoWA!  A lot of the rest of the time was spent either playing Xbox or wandering round the few other exhibitors - I was impressed with <a href="http://yahoo.co.uk">Yahoo!</a>'s presence and will certainly consider going along to one of their YDN evenings, <a href="https://www.x.com/blog/">PayPal's new, better API</a> looked pretty neat, and a few of the smaller companies like <a href="http://go-test.it/">Go Test It</a> impressed with the things they were creating.</p>

<p>See you next year!</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[XHTML not dead, despite reports]]></title>
            <link href="/xhtml-not-dead-despite-reports"/>
            <updated>2009-07-20T00:00:00+01:00</updated>
            <id>/xhtml-not-dead-despite-reports</id>
            <content type="html"><![CDATA[<p>With the W3C's recent <a href="http://www.w3.org/News/2009#item119">announcement that work on XHTML 2.0 is not being continued</a>, it would be tempting to think that the HTML vs XHTML war has been 'won', and not by the side a lot of people wanted.</p>

<p>However, that's a misconception.  XHTML is alive and well as part of <a href="http://www.w3.org/TR/html5/">HTML5</a>, on more or less equal terms with 'plain' HTML.  It's just not going to be replacing 'tag soup' any time soon unless people start using it!</p>

<p>I'll be taking a look at the options authors have for producing XHTML markup, but lets first look at why someone might want to use XML.</p>

<h4>Pros and cons of XHTML</h4>

<p>Pros:</p>

<ul>
<li>Parsing - XHTML pages can be parsed using standard XML toolkits.</li>
<li>Transforms - pages can have XSL transforms applied to them.</li>
<li>Embedded content - pages can have other XML namespaces embedded into them.</li>
<li>Unambiguous - the author doesn't have to remember whether a tag needs closing or not, or whether the tag should be upper or lowercase.  This also taps into the general developer's 'coolness' gene.</li>
<li>XML is 'cool'.</li>
</ul>

<p>Cons:</p>

<ul>
<li>Brittleness - it's a lot easier to make a mistake that renders the page invalid XHTML.</li>
<li>Deprecated JS - you can't use some constructs like document.write().</li>
<li>No iFrames - these aren't supported in XHTML strict.</li>
<li>MIME type - there is confusion about which type XHTML should be served as (see next section).</li>
</ul>

<p>To be honest, it is worth considering whether those pros are worth it for you.  There are plenty of people who think you should just use HTML 4.01 and not worry about XML.</p>

<h4>Option 1: Use XHTML 1.1 and serve it as text/html</h4>

<p>Just because XHTML 2.0 has gone away, it doesn't mean you have to jump on board XHTML 5 - browsers will continue supporting XHTML 1.1 for decades to come.</p>

<p>There are a few caveats to consider when using XHTML but serving it as HTML. The <a href="http://www.w3.org/TR/xhtml-media-types/#text-html">W3C XHTML Media Types recommendation</a> states:</p>

<blockquote>
"In general, this media type is NOT suitable for XHTML except when the XHTML is conforms to the guidelines in Appendix A. In particular, 'text/html' is NOT suitable for XHTML Family document types that add elements and attributes from foreign namespaces, such as XHTML+MathML [...] XHTML documents served as 'text/html' will not be processed as XML"
</blockquote>

<p>In short, even if your DOCTYPE says your document is XHTML 1.1 Strict, your browser will not believe you and will basically parse it as HTML.  There is good reason for this -however: surveys suggest that less than 40% of documents online validate against the doctype they declare.</p>
 
<p>Anecdotally, the Microformats community has developed a number of proxy services that take HTML pages as input and transform them to other document types, and nearly all of them have run into the problem of supposedly-XHTML documents causing errors when fed into XSL engines.  Most now run content through Tidy before processing.</p>

<p>This option means:</p>

<ul>
<li>Your pages have to be valid HTML 4.01 anyway, by using the Appendix A compatibility guidelines.</li>
<li>You can't use any of the XML features like namespaces.</li>
<li>Consumers are advised not to trust that it's XML anyway, so browser will render it the same as HTML 4.01.</li>
<li>Consequently, this option is best thought of as a subset of HTML 4.01 where the document just happens to also be valid XML.</li>
</ul>

<h4>Option 2: Use XHTML 1.1 served as application/xhtml+xml</h4>

<p>The <a href="http://www.w3.org/TR/xhtml-media-types/#text-html">same W3C document quoted above</a> says:</p>

<blockquote>
"Family documents. 'application/xhtml+xml' should be used for serving XHTML documents to XHTML user agents (agents that explicitly indicate they support this media type)."
</blockquote>

<p>In short, even if you're serving documents as text/html to some clients, you should use application/xhtml+xml for those that say they can support it, like most modern browsers except IE.  Very few sites actually do this.</p>

<p>The reason is that when served as application/xhtml+xml, browsers actually trust that your document is going to be XML and throw valiation errors or break when it's not.  Why is that bad? Well, it turns out it's actually pretty hard to guarantee this, even at the server side.</p>

<p>Developers are reasonably conversant in XML syntax, but does that apply to everyone that gets to generate content on your site?  Maybe there's a junior front-end developer who knows how to bash HTML in Dreamweaver, maybe users can post content onto the site themselves, or maybe your super-genius senior developers will occasionally make a typo.</p>

<p>Essentially the upshot of this option is:</p>
<ul>
<li>If you do this for some clients, you still have to do Option 1 for other users, which means you either have to do everything twice, or you have to duplicate your work.</li>
<li>You have to do a lot more work ensuring that your markup is valid XML at the server side.</li>
<li>You do get to use XML-specific features, as long as you don't mind not serving your content to some users.</li>
</ul>

<h4>Option 3: Use XHTML5</h4>

<p>At this point in the post, you're probably thinking that XHTML is a bit of a mess, and might be hoping that I'm going to say that HTML5 solves all the problems.  Sadly, that's not the case.</p>

<p>HTML5 does however clean the issues up somewhat. </p> 

<p>For a start, unlike HTML 4.01 vs XHTML 1.1, the XML element of HTML5 is in the core of the specification.  The HTML5 spec (or proposed spec, I should say) defines the elements and attributes in terms of a parsed DOM, and then explains how they should be serialised into XML and 'HTML' forms.

<p>The spec also goes to great lengths to specify a parsing and error-handling model for the HTML serialisations, so that browsers can know the 'right' way to parse seemingly-malformed content like &lt;p&gt;&lt;b&gt;&lt;/p&gt;&lt;/b&gt;> and get a consistent result.  This may seem like a waste of time in a world where XML exists, but in reality with so many hand-coded sites out there, it's a good idea to have a consistent set of rules about how to handle bad markup.</p>

<p>One other way that HTML5 clarifies the split between HTML and XHTML is that the two document types share a consistent DOCTYPE:</p>

<p>&lt;!DOCTYPE html&gt;</p>

<p>Think about the current generation's situation:</p>
<ul>
<li>Documents served as text/html with the HTML 4 doctype are HTML</li>
<li>Documents served as text/html with the XHTML 1.1 doctype are HTML and valid XHTML</li>
<li>Documents served as application/xhtml+xml and the XHTML 1.1 doctype are XHTML</li>
</ul>

<p>By having a single DOCTYPE, HTML5 avoids the awkward middle situation:</p>
<ul>
<li>Documents served as text/html with the HTML5 doctype are HTML</li>
<li>Documents served as application/xhtml+xml with the HTML docytpe are XHTML</li>
</ul>

<p>This option seems like a good one going forwards, as HTML5 elements become more and more supported by browsers:</p>
<ul>
<li>Authors who can guarantee their pages are valid XML can serve them as XHTML to supporting browsers.</li>
<li>Authors who think their pages are probably XML can serve them as text/html and if any mistakes sneak in, it won't matter too much.  The HTML parsing engine in HTML5 should render their documents as intended anyway.</li>
<li>Authors who don't want to worry about XML can write their pages as HTML and continue not to care about whether they're parsable by XML toolkits, as they know that the content-type and DOCTYPE they're using offer no such guarantees to consumers.</li>
</ul>

<h4>The future</h4>

<p>My own feeling is that the XHTML 1.1 specification didn't bring much to the table for browser manufacturers.  It defined a new MIME type for them to recognise, and started asking them to validate pages as XML in some instances, but didn't actually add much in terms of useful features - browsers already had an HTML rendering engine after all, so it's not as if XHTML made anything easier for them.</p>

<p>Also from the development side, XHTML was implemented on sites for a few different motivations: a sense of neatness, the wish to keep up with the latest thing, but there was rarely a business case for its implementation - very few sites rely on their output being parsable as XML.</p>

<p>Hopefully by having XML at the core of the new specification, XHTML will be more and more embraced.  When user-agents are upgraded to parse the new markup, programming teams will also feel the need to implement the XML parser as well as the HTML parser.  Similarly, when sites are redesigned to be HTML5-compatible, hopefully in a lot of cases XML-compatibility will be included in that work.</p>

<p>However, in another sense hopefully HTML5's strong specification of how HTML should be parsed will enable browser manufacturers to further standardise how they treat 'tag soup', and maybe those amateur hand-coders out there will find that their non-validating badly-written code is at least achieving the goal of working roughly the same no matter what browser looks at it.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Using Twitter as a voting platform]]></title>
            <link href="/using-twitter-as-a-voting-platform"/>
            <updated>2009-06-15T00:00:00+01:00</updated>
            <id>/using-twitter-as-a-voting-platform</id>
            <content type="html"><![CDATA[<div class="figure narrow">
<a href="http://www.flickr.com/photos/ciaranmcnulty/3627788423/" title="Cast of Star Trek by CiaranJMcNulty, on Flickr"><img src="http://farm4.static.flickr.com/3406/3627788423_c757c36050_m.jpg" width="240" height="160" alt="Cast of Star Trek" /></a>
</div>

<p>Like a lot of other people, I've had a love/hate relationship with <a href="http://twitter.com">Twitter</a>.  At first I didn't see the point - it was just <a href="http://facebook.com">Facebook</a> without the features, then I drank the kool-aid, and fell in love with its simplicity and openness.  Nowadays I've backed off a bit and see it as an interesting social phenomenon that I enjoy being a part of. I'd been meaning to check out the <a href="http://framework.zend.com/manual/en/zend.service.twitter.html">Zend_Service_Twitter</a> PHP library for a while, but hadn't really thought of an excuse.</p>

<p>  A few weeks back I was unlucky enough to watch Star Trek V and <a href="http://twitter.com/CiaranMcNulty/status/1981914567">tweeted about how crap it was</a>, despite my friend Nick thinking it's the best of the lot. There was a bit of back and forth, so I posted an order for the films, from best to worst.  A few of my friends then did the same, with the hashtag of #startrekrank as a way of identifying the posts.</p>

<p>It struck me that I could somehow aggregate these results using the Twitter API, so I present to you, <a href="http://startrekrank.com/">StarTrekRank.com</a>!</p>

<p>Actually building the site was pretty straightforward.  The Twitter API combined with the Zend library make it pretty simple to perform searches.  I'm generating the rankings using the <a href="http://en.wikipedia.org/wiki/Condorcet_method#Kemeny-Young_method">Kemeny-Young method</a>, which is fairly processor-intensive - I suspect it's O(n!) for the number of movies - but to my mind produces very fair results.</p>

<p>The way the site operates is to periodically search for #startrekrank via the API, and index any new tweets.  Then if there are some new tweets, the overall ranking is re-calculated automatically.  Finally, the new tweeters get a message sent @them from <a href="http://twitter.com/startrekrank">the 'startrekrank' twitter account</a> thanking them and directing them back to the main site to view the results.</p>

<p>As a voting platform, Twitter is pretty efficient - voters are authenticated via their Twitter account so you know who the votes have come from.  It's only real downside is that a secret ballot isn't very practical. Now clearly this is all just a bit of fluff, but to me it's an interesting illustration of how Twitter's openness and simplicity allow more complex applications to be built on top of it. </p>]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Webmasters: opt out of Phorm now!]]></title>
            <link href="/webmasters-opt-out-of-phorm-now"/>
            <updated>2009-04-20T00:00:00+01:00</updated>
            <id>/webmasters-opt-out-of-phorm-now</id>
            <content type="html"><![CDATA[<p>I just got the following email:</p>

<blockquote>"Thank you for your submission to the Phorm website exclusion list. If there are no obvious grounds to doubt the legitimacy of the request the URL will be blocked as soon as possible, usually within 48 hours."</blockquote>

<p>You can get one too by writing an email to <a href="mailto:website-exclusion@webwise.com" class="vcard">the <span class="fn org">Phorm</span> opt-out address (<span class="email">website-exclusion@webwise.com</span>)</a> asking them to remove their service from any and all domains you have control over.  I could explain why, but it's best if you just do it first.  What you'll be doing is placing a vote against a fairly insidious new marketing system that most of your users won't know how to opt out of, or even know is happening. Go on, do it before you read the next paragraph.</p>

<p>For years marketing companies have been offering consumers the following deal: <em>"Give us a completely history of every website you visit, and in exchange we'll give you slightly more targeted adverts"</em>.  Most people on hearing this proposition don't really see what's in it for them, and decline.  Basically, the price in privacy is worth far less to most people than some idea of amazingly personalised advertising.</p>

<p>It's been tried time and time again - <a href="http://www.doubleclick.com/">DoubleClick</a> are probably the most successful attempt to push this model, they essentially operate by providing ads to different sites from their one domain, and getting each ad to set a cookie in the user browser so the browsing behaviour across sites can be tracked.  This is so unappealing that a whole ecosystem of third-party browser extensions and cookie blockers sprang up.  You could interpret that as a comprehensive rejection of the model by 'the industry'.</p>

<p>Phorm is a technology that tries to apply this model again, but at the ISP level.  The idea is that someone like BT can run all their user traffic through a logging proxy and then use that data to feed into what ads people see (it's <a href="http://en.wikipedia.org/wiki/File:Phorm_cookie_diagram.png">actually more complex than that</a>, but that's the gist of the idea).  Unlike Doubleclick, the data on the users preferences therefore comes from all sites they visit, not just ones that are part of the advertising network.  This won't be anything users notice, and its legality is being debated in different territories, but here in the UK <a href="http://www2.bt.com/static/i/btretail/webwise/">BT</a>, <a href="http://www.virginmedia.com/customers/webwise.php">Virgin Media</a> and <a href="http://www.techwatch.co.uk/2008/03/12/talk-talk-offer-phorm/">TalkTalk</a> are already trialling it under the name Webwise.</p>

<p>In theory this is an 'opt in' system for users, but it's being sneaked into the terms of service for these ISPs, as part of the big pack you get when you sign up that nobody ever reads or as one of those 'updates' you sometimes get in the post.  They'll also have an 'opt out' form buried somewhere in their support website, but for most users the whole thing will go on without their noticing.</p>

<p>As a website owner, the only way of opting out is to write to the email address I mention above.  The Phorm docs say the only way of stopping them using traffic information from visits to your site is to add the following to robots.txt:</p>

<p><pre>User-agent: *
Disallow: / </pre></p>

<p>That's right - their official solution is for you to ban <em>all</em> robots from visiting, so you have to sacrifice your search engine rankings if you want to opt out.  Perhaps a more convenient fix would be for Phorm to let us know what their user-agent's name is, but it's of course not in their interest to make it easy.</p>

<p>Phorm is currently being challenged in the courts, but while that's going on I'd urge all website owners to opt out. You'll be doing your users a favour, and you'll be adding your voice to the many who are telling ISPs that this is something we Do Not Want.</p>

]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Make all your sites work in IE8 with one fell swoop]]></title>
            <link href="/make-all-your-sites-work-in-ie8-with-one-fell-swoop"/>
            <updated>2009-04-17T00:00:00+01:00</updated>
            <id>/make-all-your-sites-work-in-ie8-with-one-fell-swoop</id>
            <content type="html"><![CDATA[<p>At work we have around 100 sites hosted for clients, some of which might not have been updated in a few years (I should point out these are sites we develop, so there's no chance a client's going to edit the site themselves).  <a href="http://blogs.msdn.com/ie/archive/2009/04/10/prepare-for-automatic-update-distribution-of-ie8.aspx">IE8 is going to be rolled out to Windows users with Automatic Updates enabled</a> as of next week, so there's a small worry about auditing these sites in time.</p>

<p>When IE7 came out we had to spend the time going through each of them manually and checking everything was fine.  This time around,<a href="http://msdn.microsoft.com/en-us/library/cc288325(VS.85).aspx"> although IE8 has a new rendering model, it's possible for the browser to render pages as if it was IE7</a>.  In general this has been hugely controversial, but for people in our situation it's pretty handy.</p>

<p>The easiest solution to having sites that may not work in the IE8 renderer is 'do nothing'.  IE8 has a compatibility button that a user can press that renders the page as if it's IE7.  If enough users press this button, a scary centralised Microsoft database marks you as a naughty site and from then on, IE8 users get to see you in 'compatibility mode' until some time in the future when you fix your site and manage to persuade Microsoft that you should be let back in to the halls of the worthy.</p>

<p>However that sounds like a mess, relies on users jumping through some hoops, and might be a bit tricky to get off the list at a later date.  The strategy we've decided to go for is to explicitly mark all our sites as needing to be rendered in compatibility view, then turn this off for each site in turn as they're audited, at our leisure.</p>

<p>The solution that Microsoft have provided is a new HTTP header, <tt>X-UA-Compatible</tt>.  The semantics of the header have been defined in a way that allows browsers aside from IE to use it but, frankly, I'll be quite upset if they do.</p>

<p>To mark an HTML page as needing to be rendered in IE7 mode, you'd include the following META tag as the first element in its HEAD, telling it to behave as IE7 would.  I've seen some advice online saying the value should be <tt>IE=IE7</tt>, but that will instruct the browser to use IE7 Standards Mode which  isn't ideal. The value I've used here, <tt>IE=EmulateIE7</tt> will tell IE8 to use either IE7 Standards Mode or IE7 Quirks Mode based on the page's <tt>&lt;!DOCTYPE&gt;</tt> using the same algorithm as IE7.</p>

<p><code><pre>&lt;meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7" /&gt;</pre></code></pre>

<p>So there's one solution - go through all the sites and add that tag to the pages. I don't like it though, for a few reasons:</p>

<ol>
<li>It's still fairly labour-intensive to open a load of old sites, work out their templating system and add in the markup.</li>
<li>This tag would be invalid in HTML5 and I don't want to get into bad habits.</li>
<li>It feels like the HTML is being cluttered up.</li>
</ol>

<p>What we've gone with in our setup is to include this instruction as a real HTTP header in our Apache httpd.conf for all sites:</p>

<p><code><pre>Header set X-UA-Compatible: IE=EmulateIE7</pre></code></p>

<p>Now, all of our sites in one go should use the compatibility view.  Then, as we audit them we can add the following to the VirtualHost for each site:</p>

<p><code><pre>Header unset X-UA-Compatible
Header set X-UA-Compatible: IE=IE8 ; optional</pre></code></p>

<p>Once we've tested each site, this will then tell IE8 to render using IE8's engine no matter what the compatibility database says.  You could leave off the header completely for sites, of course, but then you'll end up mired in the compatibility database. You would save the slight overhead of having an IE-specific header, though.</p>]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Rev-canonical should be handled with care]]></title>
            <link href="/rev-canonical-should-be-handled-with-care"/>
            <updated>2009-04-14T00:00:00+01:00</updated>
            <id>/rev-canonical-should-be-handled-with-care</id>
            <content type="html"><![CDATA[<p>A short while back, Google and a bunch of other search engines launched @rel="canonical", a standard for specifying that the current page is a copy of another, more canonical, version hosted elsewhere. <a href="http://ciaranmcnulty.com/blog/2009/02/rel-canonical-should-be-handled-with-care">I blogged about it at the time</a> , and generally approved of the idea but warned against overuse when an HTTP redirect might be more sensible.</p>
  
<p>Recently there's been a large amount of discussion about <a href="http://simonwillison.net/2009/Apr/11/revcanonical/">@rev="canonical"</a> , a proposal that seems to have been floated with the intention of providing <a href=http://revcanonical.appspot.com/ id=zueh title="URL shortening services">URL shortening services</a>. The idea is that my page can 'advertise' some other URLs that it can be found at so that clients can pick a different one to use when referring to it.</p>

<p>In this particular use case I could publish a page at <strong><tt>http://ciaranmcnulty.com/blog/2009-04-14/a-long-blog-post-with-a-complex-url</tt></strong> that had a @rel="canonical" link to <strong><tt>http://ciaran.ws/complex</tt></strong> (I don't really have that domain, don't bother trying it). Applications that wanted a shorter URL for the content (e.g. Twitter clients, SMS gateways) could then use my shorter URL rather than having to get a more obfuscated one from <a href="http://tinyurl.com">TinyURL</a> or somewhere similar.</p>

<p>The number of sites that have already included the markup is staggering in such a short time, and a testament to how a simple markup idea like this can really take off (if only Microformats could gain this kind of uptake!). I've been reading a lot of the commentary that's bouncing around the HTML blogosphere, and thought I'd put my &pound;0.02 in. Frankly, I fail to see the point of all the hooh-hah, for the following reasons:</p>

<h4>@rev is deprecated.</h4>


<p>@rev has been taken out of the proposed HTML5 specification because it's confusing and under-used, so this is probably the worst possible time to start a wide-ranging deployment of a new @rev value. The widespread use will have one of two results, either it'll be completely invalidated when HTML5 is finalised, or the deployment will cause @rev to be put back into the HTML5 spec. Either of these are bad results in my opinion.</p>

<p>The reasons @rev was taken out of the HTML5 proposal are that basically:</p>

<ol>
  <li>Most uses of @rev turned out to be typos of @rel.</li>
  <li>It was pointed that every @rev value could be turned into a @rel just by changing the keyword to indicate a reverse relation, e.g. @rev="parent" and @rel="child" are equivalent.</li>
</ol>

<p>On this basis, a better alternative to @rel="canonical" could be @rel="non-canonical" or something equally trivial - this could also be combined with @rel="alternate".</p>

<h4>Using @rev="canonical" for redirect URLs is wrong</h4>

<p>The idea of @rel="canonical" is to let search engines know that you have duplicate content at other URLs, and which version is the 'correct' one that they should be concentrating on including in their indexes. By that logic, @rev="canonical" should be a list of other URLs at which the same content as the current page exists, but would indicate to a search engine that the current URL is the one that should be used canonically. As an interesting use-case, a search engine could make indexing those URLs low-priority, or just ignore them completely.</p>

<p>However, redirect URLs don't fit in with this usage. They're resources that will redirect to the current one, not resources that contain the same information. The distinction might seem like hair-splitting but I feel it's important that @rel="canonical" is seen as for situations where there are concrete individual pages at differenet URLs.</p>

<p>On a related note, my friend Simon also has some strident opinions about 301 MOVED redirects from URLs that never initially hosted any content that I wish he'd blog about (hint hint!).</p>

<h4>There are better semantics for URL shortening than @rev="canonical"</h4>

<p>OK, so there's probably a use case for saying 'these other URLs have the same content as this page', but nearly all of the discussion has been concentrated on URL shortening.&nbsp; If we're going to use a head LINK to advertise a shorter URL for our content, there has to be a better way than saying 'these other URLs contain the same content' and letting the client check the length of each.</p>

<p>I don't really know what to propose, but something like @rel="shorter-url" or @rel="short-url" or similar would seem to be sensible.&nbsp; Anything's fine as long as it's widespread and gets registered.&nbsp; It'd be nice if someone could knock up an HTML profile for us to use too, but they seem to be on the way out.</p>

<p>Overall, the rev-canonical thing seems to be a fairly simple idea, with a few flaws, that's been overhyped and suddenly implemented everywhere with not much thought going into it.&nbsp; It may well achieve a few things though:</p>

<ol>
  <li>It's got people talking and thinking about @rel values, which is a good thing and might lead to more uptake of technologies like <a href="http://microformats.org/wiki/XFN">XFN</a>.</li>
  <li>It's prompted a lot of discussion about HTML semantics, which is a good thing and could help promote <a href="http://microformats.org/wiki/posh">POSH</a> and <a href="http://microformats.org">Microformats in general</a>.</li>
  <li>It's shown people how easy it can be to roll out a simple semantic HTML change on a large site, which has to be a good thing.</li>
</ol>

<p>I can only hope that that outweighs all the niggles I have with how rev-canonical is used, and frankly it's currently being used in such a narrow use-case that it doesn't really have a huge impact on the way we use the web.</p>

<p>[EDIT: Just as I posted this, <a href=http://annevankesteren.nl/2009/04/rev-canonical id=ry8m title="Anne van Kesteren made the same points as me">Anne van Kesteren made the same points as me</a> , but in about 10% of the words and with none of the waffle.]</p>]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Converting HTML to PDF using wkhtmltopdf]]></title>
            <link href="/converting-html-to-pdf-using-wkhtmltopdf"/>
            <updated>2009-04-09T00:00:00+01:00</updated>
            <id>/converting-html-to-pdf-using-wkhtmltopdf</id>
            <content type="html"><![CDATA[<p>I blogged a while back about <a href="http://ciaranmcnulty.com/blog/2008/08/delivering-pages-as-pdf-using-php">delivering pages as PDF using PHP</a>, and at the time DOMPDF seemed to be the best-of-breed package for converting HTML into PDF for the purposes of delivering PDF versions of web content.</p>

<p>However, I noted at the time that DOMPDF's last release was in July 2007, and it still doesn't look like being updated any time soon. The fundamental problem with packages like DOMPDF is that they tend to implement their own rendering engine.  The thing is, HTML and CSS are both pretty huge now - writing a rendering engine that can cope with all the different combinations is a huge task, so projects like DOMPDF end up missing out important bits of functionality.</p>

<p>A better approach would be to use an existing rendering engine from a browser, and then build a binary around it that can take a website as input and produce a PDF as output.  That way you can get results consistent with how browsers would print a page and if you pick the right engine you'll not have to keep up with any changes to HTML standards, the engine developers will do that for you.</p> 

<p>This is essentially the approach <a href="http://code.google.com/p/wkhtmltopdf/">wkhtmltopdf</a> takes: it extracts the open-sourced Webkit renderer used inside browsers like Safari and Chrome and bundles it up into a Linux CLI application which produces some pretty impressive results.</p>

<p>I thought I'd jump right in and start by compiling it on my Debian webserver.  The wkhtmltopdf site has some instructions for building it on Ubuntu, which I thought were worth a try.  The basic procedure was as follows:</p>

<p><code><pre>#apt-get update
#apt-get install libqt4-dev qt4-dev-tools build-essential cmake

#svn checkout http://wkhtmltopdf.googlecode.com/svn/trunk/ wkhtmltopdf
#cd wkhtmltopdf
#cmake -D CMAKE_INSTALL_PREFIX=/usr .
#make
#sudo make install</pre></code></p>

<p>In my case, this installed a terrifying amount of new packages to my server, but everything went very smoothly.  I was left with a binary in /usr/bin and ploughed right in!</p>

<p><code><pre>#wkhtmltopdf http://ciaranmcnulty.com /tmp/ciaranmcnulty.pdf
wkhtmltopdf: cannot connect to X server</pre></code></p>

<p>Argh.  The rendering engine depends on there being a GUI running on the machine so it can do cool things like generate graphics, render fonts and so forth.  A typical webserver won't be running X, but luckily there are ways around it.  </p>

<p>One such way is <a href="http://www.x.org/archive/X11R6.8.1/doc/Xvfb.1.html">xvfb</a>, or the X Virtual Frame Buffer.  This is a handy bit of code that basically runs an X instance but without a lot of the overheads.  You can create a temporary X buffer and run a command in it using the xvfb-run binary, the benefit of which is that the x instance gets thrown away afterwards.  I installed xvfb and then invoked it as follows:</p>

<p><code><pre>#apt-get install vfb
#xvfb-run -a -s "-screen 0 640x480x16" wkhtmltopdf --dpi 200 
  --page-size A4 http://ciaranmcnulty.com /tmp/ciaranmcnulty.pdf</pre></code></p>

<p>The options should be fairly self-explanatory, the key things to note are that -a makes xvfb pick an unused display number (to avoid collisions) and -screen starts up the virtual framebuffer with a display with the correct bit depth and dimensions.</p>

<p>The results are fairly good, certainly better than PHPDOM would generate given the same input.  My site layout uses a fair bit of floating and absolute positioning, and the PDF came out exactly as I'd expect:</p>

<p><a href="http://www.flickr.com/photos/ciaranmcnulty/3426661909/" title="Website PDF by CiaranJMcNulty, on Flickr"><img src="http://farm4.static.flickr.com/3375/3426661909_46aa0e293d.jpg" width="500" height="500" alt="Website PDF" /></a></p>

<p>It's important to note that this isn't a bitmap, the text in the PDF is still 'text'.</p>

<p>A quick dig around showed that to print the backgrounds I'd need to have Qt4.5 installed, something I wasn't really prepared to risk my server for.  However, I thought I'd quickly try doing what I should have in the first place.  The wkhtml project provides a linux binary that's statically compiled against Qt.</p>

<p>I downloaded this binary and gave it a whirl.  The results were much better:</p>

<p><a href="http://www.flickr.com/photos/ciaranmcnulty/3426662113/" title="Website PDF with backgrounds by CiaranJMcNulty, on Flickr"><img src="http://farm4.static.flickr.com/3339/3426662113_f5de7809fe.jpg" width="500" height="500" alt="Website PDF with backgrounds" /></a></p>

<p>Frankly I think this is a great rendition of the page, and certainly good enough for an autogenerated PDF on a website.  A bit of further investigation and experimentation has left me pretty impressed with the breadth of CSS print functionality webkit can support.</p>

<p>The next step for me is going to be to try and replace some of the DOMPDF installations in some of my smaller sites, and see how it performs under load. The time taken to generate a PDF is pretty high, and I've not really checked out how xvfb is with concurrency so I'd hesitate to throw it onto a production site straight away, but it'll be my first port of call next time I want to do something with a PDF.</p>
]]></content>
        </entry>
    </feed>